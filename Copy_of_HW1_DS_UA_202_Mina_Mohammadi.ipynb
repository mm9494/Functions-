{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of HW1 DS-UA 202 Mina Mohammadi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mm9494/Functions-/blob/main/Copy_of_HW1_DS_UA_202_Mina_Mohammadi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43DBqagWutAE"
      },
      "source": [
        "# Responsible Data Science Spring 2021: Homework 1\r\n",
        "\r\n",
        "\r\n",
        "This notebook contains sample code; other methods at arriving at the correct answer were also accepted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BAY1cNGuxfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "533a11ff-4ffa-4512-cc24-898a0ae92f4c"
      },
      "source": [
        "!pip install aif360==0.3.0 \r\n",
        "!pip install BlackBoxAuditing\r\n",
        "!pip install tensorflow==1.12.0\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "\r\n",
        "import random\r\n",
        "random.seed(6)\r\n",
        "\r\n",
        "import sys\r\n",
        "import warnings\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, mean_squared_error, explained_variance_score\r\n",
        "from math import sqrt\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "\r\n",
        "from aif360.algorithms.preprocessing import DisparateImpactRemover\r\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing\r\n",
        "from aif360.algorithms.inprocessing.meta_fair_classifier import MetaFairClassifier\r\n",
        "import BlackBoxAuditing\r\n",
        "from aif360.algorithms.preprocessing import Reweighing\r\n",
        "from aif360.datasets import CompasDataset, AdultDataset, StandardDataset, BankDataset, GermanDataset\r\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\r\n",
        "from aif360.algorithms.postprocessing import EqOddsPostprocessing\r\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult\r\n",
        "\r\n",
        "\r\n",
        "from aif360.algorithms.inprocessing import GerryFairClassifier\r\n",
        "from aif360.algorithms.inprocessing import PrejudiceRemover\r\n",
        "\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\r\n",
        "\r\n",
        "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aif360==0.3.0 in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360==0.3.0) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.7/dist-packages (from aif360==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360==0.3.0) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360==0.3.0) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360==0.3.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360==0.3.0) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->aif360==0.3.0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360==0.3.0) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: BlackBoxAuditing in /usr/local/lib/python3.7/dist-packages (0.1.54)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (3.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->BlackBoxAuditing) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->BlackBoxAuditing) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (0.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx->BlackBoxAuditing) (4.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->BlackBoxAuditing) (1.15.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.12.0 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 1.15.0rc0, 1.15.0rc1, 1.15.0rc2, 1.15.0rc3, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0a0, 2.0.0b0, 2.0.0b1, 2.0.0rc0, 2.0.0rc1, 2.0.0rc2, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0rc0, 2.1.0rc1, 2.1.0rc2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0rc0, 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.2.2, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.3.2, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4, 2.4.0, 2.4.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tensorflow==1.12.0\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW83Vu3KvTQB"
      },
      "source": [
        "# Problem 2\r\n",
        "### **Load and split data into train, validation and test sets** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIF2pjTqv0tS"
      },
      "source": [
        "### Read in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyEvMjuSvWP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91bfa842-4a11-4cb8-e008-9847fa079ca5"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\r\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\r\n",
        "!wget\thttps://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-07 18:26:28--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3974305 (3.8M) [application/x-httpd-php]\n",
            "Saving to: ‘adult.data.4’\n",
            "\n",
            "adult.data.4        100%[===================>]   3.79M  9.23MB/s    in 0.4s    \n",
            "\n",
            "2021-03-07 18:26:29 (9.23 MB/s) - ‘adult.data.4’ saved [3974305/3974305]\n",
            "\n",
            "--2021-03-07 18:26:29--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2003153 (1.9M) [application/x-httpd-php]\n",
            "Saving to: ‘adult.test.4’\n",
            "\n",
            "adult.test.4        100%[===================>]   1.91M  5.14MB/s    in 0.4s    \n",
            "\n",
            "2021-03-07 18:26:30 (5.14 MB/s) - ‘adult.test.4’ saved [2003153/2003153]\n",
            "\n",
            "--2021-03-07 18:26:30--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5229 (5.1K) [application/x-httpd-php]\n",
            "Saving to: ‘adult.names.4’\n",
            "\n",
            "adult.names.4       100%[===================>]   5.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-07 18:26:30 (116 MB/s) - ‘adult.names.4’ saved [5229/5229]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4UUEvEuwGZc"
      },
      "source": [
        "!cp adult.data /usr/local/lib/python3.7/dist-packages/aif360/data/raw/adult/\r\n",
        "!cp adult.test /usr/local/lib/python3.7/dist-packages/aif360/data/raw/adult/\r\n",
        "!cp adult.names /usr/local/lib/python3.7/dist-packages/aif360/data/raw/adult/"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Fqnm-byb-9y9",
        "outputId": "b29a7f0d-d640-4da1-ca7c-e486bad34753"
      },
      "source": [
        "# loading dataset \n",
        "dataset_orig = AdultDataset(protected_attribute_names=['sex'],\n",
        "                            privileged_classes=[['Male']],\n",
        "                            features_to_drop = ['race', 'age'])\n",
        "\n",
        "# creating privlidged and unpriviledged groups \n",
        "privileged_groups = [{'sex': 1}]\n",
        "unprivileged_groups = [{'sex': 0}]\n",
        "\n",
        "# splitting original data into train and test data \n",
        "train_orig, test_orig = dataset_orig.split([0.7], shuffle=True, seed=10)\n",
        "# splitting data again into training and validation data for hypterparameter tuning \n",
        "train_orig, val_orig = train_orig.split([1/3], shuffle=True)\n",
        "\n",
        "# converting into dataframes \n",
        "train_orig_df, _ = train_orig.convert_to_dataframe()\n",
        "val_orig_df, _ = val_orig.convert_to_dataframe()\n",
        "test_orig_df, _ = test_orig.convert_to_dataframe()\n",
        "\n",
        "test_orig_df.head()\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education-num</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>workclass=Federal-gov</th>\n",
              "      <th>workclass=Local-gov</th>\n",
              "      <th>workclass=Private</th>\n",
              "      <th>workclass=Self-emp-inc</th>\n",
              "      <th>workclass=Self-emp-not-inc</th>\n",
              "      <th>workclass=State-gov</th>\n",
              "      <th>workclass=Without-pay</th>\n",
              "      <th>education=10th</th>\n",
              "      <th>education=11th</th>\n",
              "      <th>education=12th</th>\n",
              "      <th>education=1st-4th</th>\n",
              "      <th>education=5th-6th</th>\n",
              "      <th>education=7th-8th</th>\n",
              "      <th>education=9th</th>\n",
              "      <th>education=Assoc-acdm</th>\n",
              "      <th>education=Assoc-voc</th>\n",
              "      <th>education=Bachelors</th>\n",
              "      <th>education=Doctorate</th>\n",
              "      <th>education=HS-grad</th>\n",
              "      <th>education=Masters</th>\n",
              "      <th>education=Preschool</th>\n",
              "      <th>education=Prof-school</th>\n",
              "      <th>education=Some-college</th>\n",
              "      <th>marital-status=Divorced</th>\n",
              "      <th>marital-status=Married-AF-spouse</th>\n",
              "      <th>marital-status=Married-civ-spouse</th>\n",
              "      <th>marital-status=Married-spouse-absent</th>\n",
              "      <th>marital-status=Never-married</th>\n",
              "      <th>marital-status=Separated</th>\n",
              "      <th>marital-status=Widowed</th>\n",
              "      <th>occupation=Adm-clerical</th>\n",
              "      <th>occupation=Armed-Forces</th>\n",
              "      <th>occupation=Craft-repair</th>\n",
              "      <th>occupation=Exec-managerial</th>\n",
              "      <th>...</th>\n",
              "      <th>native-country=China</th>\n",
              "      <th>native-country=Columbia</th>\n",
              "      <th>native-country=Cuba</th>\n",
              "      <th>native-country=Dominican-Republic</th>\n",
              "      <th>native-country=Ecuador</th>\n",
              "      <th>native-country=El-Salvador</th>\n",
              "      <th>native-country=England</th>\n",
              "      <th>native-country=France</th>\n",
              "      <th>native-country=Germany</th>\n",
              "      <th>native-country=Greece</th>\n",
              "      <th>native-country=Guatemala</th>\n",
              "      <th>native-country=Haiti</th>\n",
              "      <th>native-country=Holand-Netherlands</th>\n",
              "      <th>native-country=Honduras</th>\n",
              "      <th>native-country=Hong</th>\n",
              "      <th>native-country=Hungary</th>\n",
              "      <th>native-country=India</th>\n",
              "      <th>native-country=Iran</th>\n",
              "      <th>native-country=Ireland</th>\n",
              "      <th>native-country=Italy</th>\n",
              "      <th>native-country=Jamaica</th>\n",
              "      <th>native-country=Japan</th>\n",
              "      <th>native-country=Laos</th>\n",
              "      <th>native-country=Mexico</th>\n",
              "      <th>native-country=Nicaragua</th>\n",
              "      <th>native-country=Outlying-US(Guam-USVI-etc)</th>\n",
              "      <th>native-country=Peru</th>\n",
              "      <th>native-country=Philippines</th>\n",
              "      <th>native-country=Poland</th>\n",
              "      <th>native-country=Portugal</th>\n",
              "      <th>native-country=Puerto-Rico</th>\n",
              "      <th>native-country=Scotland</th>\n",
              "      <th>native-country=South</th>\n",
              "      <th>native-country=Taiwan</th>\n",
              "      <th>native-country=Thailand</th>\n",
              "      <th>native-country=Trinadad&amp;Tobago</th>\n",
              "      <th>native-country=United-States</th>\n",
              "      <th>native-country=Vietnam</th>\n",
              "      <th>native-country=Yugoslavia</th>\n",
              "      <th>income-per-year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>43919</th>\n",
              "      <td>159662.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13331</th>\n",
              "      <td>184303.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22970</th>\n",
              "      <td>151835.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>773</th>\n",
              "      <td>127875.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40503</th>\n",
              "      <td>203735.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 98 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         fnlwgt  education-num  ...  native-country=Yugoslavia  income-per-year\n",
              "43919  159662.0            6.0  ...                        0.0              0.0\n",
              "13331  184303.0            4.0  ...                        0.0              0.0\n",
              "22970  151835.0            9.0  ...                        0.0              1.0\n",
              "773    127875.0            7.0  ...                        0.0              0.0\n",
              "40503  203735.0            9.0  ...                        0.0              0.0\n",
              "\n",
              "[5 rows x 98 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjZvZL7nsTv9",
        "outputId": "4ea35adf-15cd-432a-a9ee-44de9f1a79d1"
      },
      "source": [
        "#MinMaxScalar on Feature columns \n",
        "\n",
        "Min_Max_Scaler = MinMaxScaler()\n",
        "train_scaler = Min_Max_Scaler.fit_transform(train_orig_df)\n",
        "print(train_scaler)\n",
        "\n",
        "\n",
        "val_scaler = Min_Max_Scaler.transform(val_orig_df)\n",
        "print(val_scaler)\n",
        "\n",
        "test_scaler = Min_Max_Scaler.transform(test_orig_df)\n",
        "print(test_scaler)\n",
        "\n",
        "#seperate feature columns and outcome which is the income per year \n",
        "x_train = train_orig_df.drop(\"income-per-year\", axis=1)\n",
        "y_train = train_orig_df[\"income-per-year\"]\n",
        "x_val = val_orig_df.drop(\"income-per-year\", axis=1)\n",
        "y_val = val_orig_df[\"income-per-year\"]\n",
        "x_test = test_orig_df.drop(\"income-per-year\", axis=1)\n",
        "y_test = test_orig_df[\"income-per-year\"]\n",
        "\n",
        "#printing and making sure we can see the outcomes \n",
        "print(\"Outcomes: \")\n",
        "y_train.value_counts()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.12444666 0.73333333 0.         ... 0.         0.         1.        ]\n",
            " [0.20740219 0.86666667 1.         ... 0.         0.         1.        ]\n",
            " [0.13294075 0.4        0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.1856383  0.8        1.         ... 0.         0.         1.        ]\n",
            " [0.14251369 0.73333333 1.         ... 0.         0.         1.        ]\n",
            " [0.17124066 0.4        1.         ... 0.         0.         0.        ]]\n",
            "[[0.15857525 0.8        1.         ... 0.         0.         0.        ]\n",
            " [0.13291625 0.33333333 1.         ... 0.         0.         1.        ]\n",
            " [0.04963625 0.6        0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.13115878 0.6        1.         ... 0.         0.         1.        ]\n",
            " [0.12279536 0.6        0.         ... 0.         0.         0.        ]\n",
            " [0.11687845 0.86666667 1.         ... 0.         0.         0.        ]]\n",
            "[[0.10421601 0.33333333 1.         ... 0.         0.         0.        ]\n",
            " [0.12251173 0.2        0.         ... 0.         0.         0.        ]\n",
            " [0.09840454 0.53333333 1.         ... 0.         0.         1.        ]\n",
            " ...\n",
            " [0.01535768 0.26666667 1.         ... 0.         0.         0.        ]\n",
            " [0.10650511 0.53333333 1.         ... 0.         0.         1.        ]\n",
            " [0.07601844 0.6        1.         ... 0.         0.         1.        ]]\n",
            "Outcomes: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    7937\n",
              "1.0    2614\n",
              "Name: income-per-year, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaQZc9mkDBQc"
      },
      "source": [
        "# Problem 2, Part (a) \r\n",
        "### **Train a baseline Logistic Regression (LR) & Random Forest (RF) model and report metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27LLmya8lUfE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq4983HX4I2b"
      },
      "source": [
        "# Set up the logistic regression model with the given hyperparameters\n",
        "initial_lr = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
        "    \n",
        "# Fit the model using the training data\n",
        "initial_lr = initial_lr.fit(x_train, y_train, sample_weight=None)\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46ALBf1T5F5j",
        "outputId": "980e537d-aaf6-42b2-8a61-78d66e0d8644"
      },
      "source": [
        "def evaluate(model, X, y_true):\n",
        "    '''Calculates the AUC and accuracy for a trained logistic regression model'''\n",
        "    \n",
        "    # Calculate predicted values\n",
        "    y_pred = model.predict_proba(X)\n",
        "    y_pred = [row[1] for row in y_pred] \n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_true, [pred_prob >= 0.5 for pred_prob in y_pred])\n",
        "    \n",
        "    # Calculate AUC\n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    #calc RMSE\n",
        "    rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
        "    \n",
        "    #calc avergage precision \n",
        "    average_precision = average_precision_score(y_true, y_pred)\n",
        "\n",
        "    #calc explained var\n",
        "    exp_var = explained_variance_score(y_true, y_pred)\n",
        "\n",
        "    return accuracy, auc, average_precision, rmse, exp_var\n",
        "\n",
        "accuracy, auc, average_precision, rmse, exp_var = evaluate(initial_lr, x_test, y_test)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"AUC: \", auc)\n",
        "print(\"Average Precision\", average_precision)\n",
        "print(\"rmse:\", rmse)\n",
        "print(\"exp_var\", exp_var)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8017247733470922\n",
            "AUC:  0.8982743218083894\n",
            "Average Precision 0.7567895318032374\n",
            "rmse: 0.36361039072561097\n",
            "exp_var 0.37968224702097075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwaQgumTeaHK",
        "outputId": "53ecc5fa-bdba-4017-9943-05f62df701f0"
      },
      "source": [
        "\n",
        "# Set up the random forest model with the given hyperparameters\n",
        "initial_rf = RandomForestClassifier(n_estimators=10, max_depth = 3)\n",
        "\n",
        "\n",
        "initial_rf = initial_rf.fit(x_train, y_train)\n",
        "\n",
        "accuracy, auc, average_precision, rmse, exp_var = evaluate(initial_rf, x_test, y_test)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"AUC: \", auc)\n",
        "print(\"Average Precision\", average_precision)\n",
        "print(\"rmse:\", rmse)\n",
        "print(\"exp_var:\", exp_var)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7911844917815287\n",
            "AUC:  0.8673772388440433\n",
            "Average Precision 0.6786154971731467\n",
            "rmse: 0.36724072748928305\n",
            "exp_var: 0.2791915333282222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qZgybMrFsKB"
      },
      "source": [
        "# Problem 2, Part (b)\r\n",
        "### **Hyperparameter tuning of baseline LR and RF models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DIWYPazWs-4"
      },
      "source": [
        "# function for boxplots\r\n",
        "def plot_init_v_tuned_box(init_metrics, tuned_metrics, metric_name):\r\n",
        "  '''Creates a bar graph comparing init_metrics to tuned_metrics'''\r\n",
        "\r\n",
        "  # Make some x values\r\n",
        "  x_init = list(range(len(init_metrics)))\r\n",
        "  x_tuned = [x + 0.35 for x in x_init]\r\n",
        "\r\n",
        "  # Plot the metrics\r\n",
        "  plt.boxplot([init_metrics, tuned_metrics], labels=['Initial Model', 'Tuned Model'])\r\n",
        "\r\n",
        "  # Create labels, etc. \r\n",
        "  plt.ylabel(metric_name)\r\n",
        "  plt.legend()\r\n",
        "  plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzeAkGYYxm-R"
      },
      "source": [
        "#logistic regression hyperparameter tuning \n",
        "def tune_logistic_regression(train_df, val_df, penalty_types, C_values, weights=None, verbose=True):\n",
        "    '''Tunes logistic regression models over the hyperparameters penalty type and C\n",
        "       to maximize the AUC'''\n",
        "\n",
        "    # Pre-process the training and validation data\n",
        "    x_train = train_df.drop(\"income-per-year\", axis=1)\n",
        "    y_train = train_df[\"income-per-year\"]\n",
        "    x_val = val_df.drop(\"income-per-year\", axis=1)\n",
        "    y_val = val_df[\"income-per-year\"]\n",
        "\n",
        "    # Create empty lists where we will store the results of hyperparameter tuning \n",
        "    parameters = []\n",
        "    models = []\n",
        "    val_aucs = []\n",
        "    rmse = []\n",
        "    exp_var = []\n",
        "\n",
        "\n",
        "    # Loop through the hyperparameters of interest\n",
        "    for penalty in penalty_types:\n",
        "        for C in C_values:\n",
        "            \n",
        "            # Train the logistic regression model with the given hyperparameters\n",
        "            lr = LogisticRegression(C=C, penalty=penalty, solver='liblinear')\n",
        "    \n",
        "            # Fit the model using the training data\n",
        "            lr = lr.fit(x_train, y_train, sample_weight=weights)\n",
        "            \n",
        "            # Get the evalution metrics on the validation set \n",
        "            accuracy, auc, average_precision, rmse, exp_var = evaluate(lr, x_test, y_test)\n",
        "            \n",
        "            # Store the results\n",
        "            parameters.append({'penalty': penalty, 'C': C})\n",
        "            models.append(lr)\n",
        "            val_aucs.append(auc)\n",
        "            exp_var.append(explained_variance_score)\n",
        "            rmse.append(rmse)\n",
        "            \n",
        "            # Print the results\n",
        "            if verbose:\n",
        "                print(\"\\nParameters: \\tpenalty={} \\tC={}\".format(penalty, C))\n",
        "                print(\"Validation AUC: {}\".format(auc))\n",
        "            \n",
        "    \n",
        "    # Determine the best model -- that is, the one with the highest AUC\n",
        "    best_model_index = np.argmax(val_aucs)\n",
        "    best_model = models[best_model_index]\n",
        "    \n",
        "    print(\"\\nBest model parameters: \", parameters[best_model_index])\n",
        "    print(\"Best model AUC: \", val_aucs[best_model_index])\n",
        "    \n",
        "    # Return best model\n",
        "    return best_model, parameters, models, val_aucs, rmse, exp_var\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "aiwWqF1v0Nhg",
        "outputId": "e2f4a7db-2783-4d90-97c6-e90ca93aaf76"
      },
      "source": [
        "#determining the best model for linear regression\n",
        "best_model, parameters, models, val_aucs, rmse, exp_var = tune_logistic_regression(train_orig_df, val_orig_df, penalty_types=[\"l1\", \"l2\"], C_values=[0.001, 0.1, 1, 10, 100, 1000, 10000, 100000])\n",
        "\n",
        "# Training logistic regression \n",
        "best_lr = best_model.fit(x_train, y_train, sample_weight=None)\n",
        "\n",
        "accuracy, avg_precision, auc, dis_impact, mean_diff = evaluate(best_lr, x_test, y_test)\n",
        "print(\"Best tuned logistic regression model metrics\")\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Average Precision: \", avg_precision)\n",
        "print(\"AUC: \", auc)\n",
        "print(\"RMSE: \", rmse)\n",
        "print(\"Explained variance\", exp_var)\n",
        "\n",
        "initial_lr_metrics = evaluate(initial_lr, x_test, y_test)\n",
        "tuned_lr_metrics = evaluate(best_lr, x_test, y_test)\n",
        "for m in models:\n",
        "  plot_init_v_tuned_box(initial_lr_metrics, tuned_lr_metrics, \"accuracy\")\n",
        "  plot_init_v_tuned_box(initial_lr_metrics, tuned_lr_metrics, \"avg_precision\")\n",
        "  plot_init_v_tuned_box(initial_lr_metrics, tuned_lr_metrics, \"auc\")\n",
        "  plot_init_v_tuned_box(initial_lr_metrics, tuned_lr_metrics, \"rmse\")\n",
        "  plot_init_v_tuned_box(initial_lr_metrics, tuned_lr_metrics, \"exp_var\")\n",
        "\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-8c35dc590128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#determining the best model for linear regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_aucs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_orig_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_orig_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"l1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Training logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-9dc6465fc407>\u001b[0m in \u001b[0;36mtune_logistic_regression\u001b[0;34m(train_df, val_df, penalty_types, C_values, weights, verbose)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mval_aucs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mexp_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplained_variance_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Print the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PccGoqxzNvQ"
      },
      "source": [
        "#random forest hyperparameter tuning \n",
        "def tune_random_forest(train_df, val_df, penalty_types, C_values, weights=None, verbose=True):\n",
        "    '''Tunes logistic regression models over the hyperparameters penalty type and C\n",
        "       to maximize the AUC'''\n",
        "    # Pre-process the training and validation data\n",
        "    x_train = train_df.drop(\"income-per-year\", axis=1)\n",
        "    y_train = train_df[\"income-per-year\"]\n",
        "    x_val = val_df.drop(\"income-per-year\", axis=1)\n",
        "    y_val = val_df[\"income-per-year\"]\n",
        "\n",
        "    # Create empty lists where we will store the results of hyperparameter tuning \n",
        "    parameters = []\n",
        "    models = []\n",
        "    val_aucs = []\n",
        "    \n",
        "    # Loop through the hyperparameters of interest\n",
        "    for penalty in penalty_types:\n",
        "        for C in C_values:\n",
        "            \n",
        "            # Train the logistic regression model with the given hyperparameters\n",
        "            rf = RandomForestClassifier(C=C, penalty=penalty, solver='liblinear')\n",
        "    \n",
        "            # Fit the model using the training data\n",
        "            rf = rf.fit(x_train, y_train, sample_weight=weights)\n",
        "            \n",
        "            # Get the evalution metrics on the validation set \n",
        "            accuracy, auc  = evaluate(lr, x_val, y_val)\n",
        "            \n",
        "            # Store the results\n",
        "            parameters.append({'penalty': penalty, 'C': C})\n",
        "            models.append(lr)\n",
        "            val_aucs.append(auc)\n",
        "            \n",
        "            # Print the results\n",
        "            if verbose:\n",
        "                print(\"\\nParameters: \\tpenalty={} \\tC={}\".format(penalty, C))\n",
        "                print(\"Validation AUC: {}\".format(auc))\n",
        "            \n",
        "    \n",
        "    # Determine the best model -- that is, the one with the highest AUC\n",
        "    best_model_index = np.argmax(val_aucs)\n",
        "    best_model = models[best_model_index]\n",
        "    \n",
        "    print(\"\\nBest model parameters: \", parameters[best_model_index])\n",
        "    print(\"Best model AUC: \", val_aucs[best_model_index])\n",
        "    \n",
        "    # Return best model\n",
        "    return best_model, parameters, models, val_aucs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMifMyzsIZae",
        "outputId": "3b43c43d-b3e5-410f-9b17-02552040b4c7"
      },
      "source": [
        "#training the model random forest  \n",
        "\n",
        "best_model, parameters, models, val_aucs = tune_logistic_regression(train_orig_df, val_orig_df, penalty_types=[\"l1\", \"l2\"], C_values=[0.001, 0.1, 1, 10, 100, 1000, 10000, 100000])\n",
        "\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Parameters: \tpenalty=l1 \tC=0.001\n",
            "Validation AUC: 0.5991255490235945\n",
            "\n",
            "Parameters: \tpenalty=l1 \tC=0.1\n",
            "Validation AUC: 0.8987921379292803\n",
            "\n",
            "Parameters: \tpenalty=l1 \tC=1\n",
            "Validation AUC: 0.8999194688930814\n",
            "\n",
            "Parameters: \tpenalty=l1 \tC=10\n",
            "Validation AUC: 0.8987577000938417\n",
            "\n",
            "Parameters: \tpenalty=l1 \tC=100\n",
            "Validation AUC: 0.8970662971306894\n",
            "\n",
            "Parameters: \tpenalty=l1 \tC=1000\n",
            "Validation AUC: 0.8959261027932801\n",
            "\n",
            "Parameters: \tpenalty=l1 \tC=10000\n",
            "Validation AUC: 0.8955586690920659\n",
            "\n",
            "Parameters: \tpenalty=l1 \tC=100000\n",
            "Validation AUC: 0.8955439763899311\n",
            "\n",
            "Parameters: \tpenalty=l2 \tC=0.001\n",
            "Validation AUC: 0.5967512402992595\n",
            "\n",
            "Parameters: \tpenalty=l2 \tC=0.1\n",
            "Validation AUC: 0.6135499351227186\n",
            "\n",
            "Parameters: \tpenalty=l2 \tC=1\n",
            "Validation AUC: 0.6135501674184045\n",
            "\n",
            "Parameters: \tpenalty=l2 \tC=10\n",
            "Validation AUC: 0.5933305701761492\n",
            "\n",
            "Parameters: \tpenalty=l2 \tC=100\n",
            "Validation AUC: 0.6135501964553653\n",
            "\n",
            "Parameters: \tpenalty=l2 \tC=1000\n",
            "Validation AUC: 0.6135499351227187\n",
            "\n",
            "Parameters: \tpenalty=l2 \tC=10000\n",
            "Validation AUC: 0.6135498770487972\n",
            "\n",
            "Parameters: \tpenalty=l2 \tC=100000\n",
            "Validation AUC: 0.5933306282500708\n",
            "\n",
            "Best model parameters:  {'penalty': 'l1', 'C': 1}\n",
            "Best model AUC:  0.8999194688930814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5bJ5wamFt7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea366c73-edd9-4315-9e8a-ffd43927bbcf"
      },
      "source": [
        "# function for boxplots\r\n",
        "def plot_init_v_tuned_box(init_metrics, tuned_metrics, metric_name):\r\n",
        "  '''Creates a bar graph comparing init_metrics to tuned_metrics'''\r\n",
        "\r\n",
        "  # Make some x values\r\n",
        "  x_init = list(range(len(init_metrics)))\r\n",
        "  x_tuned = [x + 0.35 for x in x_init]\r\n",
        "\r\n",
        "  # Plot the metrics\r\n",
        "  plt.boxplot([init_metrics, tuned_metrics], labels=['Initial Model', 'Tuned Model'])\r\n",
        "\r\n",
        "  # Create labels, etc. \r\n",
        "  plt.ylabel(metric_name)\r\n",
        "  plt.legend()\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "print(plot_init_v_tuned_box)\r\n",
        "\r\n",
        "  "
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<function plot_init_v_tuned_box at 0x7f18c0a2b5f0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKhDj-XJRUJL"
      },
      "source": [
        "# Problem 2, Part (c) \r\n",
        "### **Disparate Impact Pre-Processing intervention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NToBvrQiWIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d6171c-cf31-406a-8042-61465403207f"
      },
      "source": [
        "def plot_repair_levels(repair_levels, metric_vals, metric_name, x_label='Repair level'):\r\n",
        "  '''Takes a list of repair levels and another list of values of a metric and\r\n",
        "  creates a line plot showing how the metric changed for different values of repair level'''\r\n",
        "\r\n",
        "  # Plot the metrics\r\n",
        "  plt.plot(repair_levels, metric_vals, color='#0384fc', linewidth=3, label=metric_name)\r\n",
        "\r\n",
        "  # Create labels, etc. \r\n",
        "  plt.xlabel(x_label)\r\n",
        "  plt.ylabel(metric_name)\r\n",
        "  plt.legend()\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "\r\n",
        "protected = 'sex'\r\n",
        "dataset_orig = AdultDataset(protected_attribute_names=['sex'],\r\n",
        "                            privileged_classes=[['Male']],\r\n",
        "                            features_to_drop = ['race', 'age'])\r\n",
        "scaler = MinMaxScaler(copy=False)\r\n",
        "\r\n",
        "test, train = dataset_orig.split([16281])\r\n",
        "train.features = scaler.fit_transform(train.features)\r\n",
        "test.features = scaler.fit_transform(test.features)\r\n",
        "\r\n",
        "index = train.feature_names.index(protected)\r\n",
        "\r\n",
        "DIs = []\r\n",
        "for level in tqdm(np.linspace(0., 1., 11)):\r\n",
        "    di = DisparateImpactRemover(repair_level=level)\r\n",
        "    train_repd = di.fit_transform(train)\r\n",
        "    test_repd = di.fit_transform(test)\r\n",
        "    \r\n",
        "    X_tr = np.delete(train_repd.features, index, axis=1)\r\n",
        "    X_te = np.delete(test_repd.features, index, axis=1)\r\n",
        "    y_tr = train_repd.labels.ravel()\r\n",
        "    \r\n",
        "    lmod = LogisticRegression(class_weight='balanced', solver='liblinear')\r\n",
        "    lmod.fit(X_tr, y_tr)\r\n",
        "    \r\n",
        "    test_repd_pred = test_repd.copy()\r\n",
        "    test_repd_pred.labels = lmod.predict(X_te)\r\n",
        "\r\n",
        "    p = [{protected: 1}]\r\n",
        "    u = [{protected: 0}]\r\n",
        "    cm = BinaryLabelDatasetMetric(test_repd_pred, privileged_groups=p, unprivileged_groups=u)\r\n",
        "    DIs.append(cm.disparate_impact())\r\n",
        "\r\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n",
            "100%|██████████| 11/11 [04:27<00:00, 24.35s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "XpDwyW6xNpAn",
        "outputId": "51620b82-374d-4474-de55-d94bb81d0520"
      },
      "source": [
        "for repair_level in [0.2, 0.4, 0.6, 0.8, 1]:\n",
        "    plot_repair_levels(repair_level, DIs, \"Disparate Impact\", x_label='Repair level')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-78ec5f753f90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrepair_level\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mplot_repair_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepair_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Disparate Impact\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Repair level'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-876937fa1341>\u001b[0m in \u001b[0;36mplot_repair_levels\u001b[0;34m(repair_levels, metric_vals, metric_name, x_label)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Plot the metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepair_levels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#0384fc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Create labels, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (11,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}